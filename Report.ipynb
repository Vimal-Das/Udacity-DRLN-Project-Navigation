{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project Report : Navigation DRLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project implements a simple agent trained using Reinforcement Learning to navigate large environment in continuous space and collect rewards (yellow bananas) whilst avoiding collisions (blue bananas).\n",
    "\n",
    "Here you can see a short clip of the trained agent in action:\n",
    "![TrainedAgent](./trained_agent.gif \"trainedagent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    " - **Navigation.ipynb**: imports all the required modules and allows to explore the environment and traind the agent.\n",
    " - **model.py**: contains Neural Network implementation based on PyTorch.\n",
    " - **dqn_agent.py**: contains the agent implementations as described in the earlier mentioned paper.\n",
    " - **checkpoint_final.pth**: contains the weights of the trained agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Learning Algorithm\n",
    "The agent implementation is based on Deep Q-Network (DQN) as described in the [Human-Level Control through Deep Reinforcement Learning paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). I had choosen to use the Deep Q-Network (DQN) because it was used in the earlier exercise and I was familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network model\n",
    "The network model I have choosen is also same as that we used in the earlier exercise.\n",
    "\n",
    "It consists of a simple pytorch model with an input layer, two hidden layers and an output layer. The size of the input layer is identical to the size of the state vector, whilst the 2nd and 3rd layer both have 64 units. The output layer has the size equal to the number of allowed agent actions. \n",
    "\n",
    "I had tried with higher number of units for the 2nd and 3rd layer, but I didnt see any noticable difference in the Agent's performance hence retained them with 64 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "The agent is based on the DQN Algorithm and hence uses two instances of the Network Model described above. One each for the local and target network which are trained as described in the earlier mentioned paper. Both networks are identical - the only difference lies in a way their weights are being updated.\n",
    " \n",
    "As the agent learns by interacting with the banana environment the weights of both networks are being updated and the resulting score is being accumulated.\n",
    "\n",
    "The agent uses a replay buffer to accumulate the agent's interactions with the environment. These experiences are then later used in the learning algorithm. By randomly sampling from the experience buffer we avoid stored experience correlations and can take advantage of rare and potentially valuable experiences, thus improving the agent's learning. This of course introduces two hyperparameters: the size of the buffer and the size of the batch. I have explained the values that I had choosen for these hyperparemeters and the reasoning in the below section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparemeters\n",
    " - **Model**:\n",
    "    - ***hidden layers*** in model: 2 (unchanged from dqn exercise)\n",
    "    - ***nodes*** in hidden layers in model: 64 (unchanged from dqn exercise. Tried with higher values but didn't result in noticable difference.)\n",
    "\n",
    " - **Agent**:\n",
    "    - ***replay buffer size (BUFFER_SIZE)*** : 100000. I chose the size of the buffer to be 100000, which is reasonably high to make room to hold valuable experiences across the training and yet small enough to fit in the RAM on my training machine.\n",
    "    - ***minibatch size (BATCH_SIZE)*** : 64. I choose a higher batch size hoping to make the training a bit faster. The higher batch size could be the reason for the larger fluctuation seen in the average scores.\n",
    "    - ***network updates (UPDATE_EVERY)*** : 4. The Q-Network is being updated every 4 steps of agents interactions with the environment.\n",
    "    - ***discount factor (GAMMA)*** : 0.99 (Let the agent focus on the long term returns instead of immediate reward)\n",
    "    - ***soft update of target parameters (TAU)*** : 1e-3 (Unchanged from dqn exericse)\n",
    "    - ***learning rate (LR)*** = 5e-4 (Unchanged from dqn exericse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "I had set a higher target if 15 for the Agent to consider the environment solved. The agent achieved the the target (15) in 756 episodes. If we look at the chart, it is evident that the agent reached the project's target score of 13 within 500 episodes. \n",
    "\n",
    "<img src=\"score_vs_episodes_table.png\" alt=\"Average scores every 100 episodes\" title=\"Score Table\" style=\"width: 600px;\"/>\n",
    "<img src=\"score_vs_episodes_plot.png\" alt=\"Plot of scores with every episode\" title=\"Score Plot\" />\n",
    "\n",
    "The **checkpoint_final.pth** file contains the trained Agent's weights which can be used to check the trained agent in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can futher improve the performance by implementing several improvements suggested in the original paper:\n",
    " - Double dqn\n",
    " - Prioritized experience replay\n",
    " - Dueling dqn\n",
    " - Rainbow (all of the above)\n",
    "\n",
    "We could also tune the existing parameters more, such as already discussed expending of experience buffer and shrinking the batch size.\n",
    "\n",
    "I also plan to work on pixel to action version of the project. This would require the network model to have convolution layers to work with the image input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
